

 OpenvSwitch QoS setting How-to 
 ==============================

  
   --------------------
   |       OVS        |
   --------------------
  (veth0) (veth2) (veth4)
    |       |       |
    |       |       |
  (veth1) (veth3) (veth5)
    |       |       |
    H1      H2      H3

    veth0: 10.1.1.2, veth1: 10.1.1.1
    veth2: 10.1.1.4, veth3: 10.1.1.3
    veth4: 10.1.1.6, veth5: 10.1.1.5


 1. prepare openvswitch installed and virtual machines or network namespace instances
    -> (c.f) netns_openvswitch_history document 

    # ip netns add ns1
    # ip link add veth0 type veth peer name veth1
    # ip link set veth1 netns ns1
    # ip netns exec ns1 ifconfig veth1 10.1.1.1/24 up
    # ifconfig veth0 10.1.1.2/24

    # ip link set veth2 netns ns2
    # ip link add veth2 type veth peer name veth3
    # ip link set veth3 netns ns2                                                                           
    # ip netns exec ns2 ifconfig veth3 10.1.1.3/24 up
    # ifconfig veth2 10.1.1.4/24 up

    # ip link set veth4 netns ns3
    # ip link add veth4 type veth peer name veth5
    # ip link set veth5 netns ns3                                                                           
    # ip netns exec ns3 ifconfig veth5 10.1.1.5/24 up
    # ifconfig veth4 10.1.1.6/24 up

    <ovs setting>
    # ovs-vsctl add-br s1
    # ovs-vsctl set bridge s1 other-config:hwaddr=00:00:00:aa:bb:cc
    # ovs-vsctl add-port s1 veth0
    # ovs-vsctl add-port s1 veth2
    # ovs-vsctl add-port s1 veth4


 2. QoS and Queue Generation

        ovs-vsctl set port veth0 qos=@newqos 
        -- --id=@newqos create qos type=linux-htb queues=0=@q0,1=@q1 \
        -- --id=@q0 create queue other-config:min-rate=1000000000 other-config:max-rate=1000000000 \
        -- --id=@q1 create queue other-config:min-rate=2000000 other-config:max-rate=2000000

        
        ** command result
        d49c34d6-bec7-44fb-b41b-fef2822e06c2 <-- uuid of qos
        2838672b-9873-4919-8f0c-b9c6d2fdfad6 <-- uuid of queue0
        94a60622-269f-49b5-a51f-09f54b458714 <-- uuid of queue1



 3.  check 

    # ovs-appctl -t ovs-vswitchd qos/show veth0
      QoS: veth0 linux-htb
      max-rate: 10000000000

      Default:
              burst: 12512
              min-rate: 1000000000
              tx_packets: 27
              tx_bytes: 1134
              tx_errors: 0

      Queue 1:
              burst: 12512
              burst: 12512
              min-rate: 2000000
              min-rate: 1000000000
              tx_packets: 0
              tx_bytes: 0
              tx_errors: 0



    # ovs-vsctl list port veth0
      _uuid               : 2aee8209-b084-4ed1-9227-793f6b4436ee
      bond_active_slave   : []
      bond_downdelay      : 0
      bond_fake_iface     : false
      bond_mode           : []
      bond_updelay        : 0
      external_ids        : {}
      fake_bridge         : false
      interfaces          : [af5c5674-ca4d-4af1-9653-c58cf7f2207d]
      lacp                : []
      mac                 : []
      name                : "veth0"
      other_config        : {}
      qos                 : d49c34d6-bec7-44fb-b41b-fef2822e06c2
      rstp_statistics     : {}
      rstp_status         : {}
      statistics          : {}
      status              : {}
      tag                 : []
      trunks              : []
      vlan_mode           : []


    # ovs-vsctl list qos
      _uuid               : d49c34d6-bec7-44fb-b41b-fef2822e06c2
      external_ids        : {}
      other_config        : {}
      queues              : {0=2838672b-9873-4919-8f0c-b9c6d2fdfad6, 1=94a60622-269f-49b5-a51f-09f54b458714}
      type                : linux-htb


    # ovs-vsctl list queue
      _uuid               : 94a60622-269f-49b5-a51f-09f54b458714
      dscp                : []
      external_ids        : {}
      other_config        : {max-rate="2000000", min-rate="2000000"}

      _uuid               : 2838672b-9873-4919-8f0c-b9c6d2fdfad6
      dscp                : []
      external_ids        : {}
      other_config        : {max-rate="1000000000", min-rate="1000000000"}


   <HTB setting check>
    # tc -s -d class show dev veth0
      class htb 1:1 parent 1:fffe prio 0 quantum 200000 rate 1000Mbit ceil 1000Mbit linklayer ethernet burst 1500b/1 mpu 0b overhead 0b cburst 1500b/1 mpu 0b overhead 0b level 0
       Sent 6309347914 bytes 4816781 pkt (dropped 0, overlimits 0 requeues 0)
       rate 0bit 0pps backlog 0b 0p requeues 0
       lended: 777469 borrowed: 0 giants: 0
       tokens: 189 ctokens: 189

      class htb 1:fffe root rate 10000Mbit ceil 10000Mbit linklayer ethernet burst 1250b/1 mpu 0b overhead 0b cburst 1250b/1 mpu 0b overhead 0b level 7
       Sent 6346637648 bytes 4841444 pkt (dropped 0, overlimits 0 requeues 0)
       rate 0bit 0pps backlog 0b 0p requeues 0
       lended: 0 borrowed: 0 giants: 0
       tokens: 17 ctokens: 17

      class htb 1:2 parent 1:fffe prio 0 quantum 25000 rate 2000Kbit ceil 2000Kbit linklayer ethernet burst 1564b/1 mpu 0b overhead 0b cburst 1564b/1 mpu 0b overhead 0b level 0
       Sent 37289734 bytes 24663 pkt (dropped 0, overlimits 0 requeues 0)
       rate 0bit 0pps backlog 0b 0p requeues 0
       lended: 12136 borrowed: 0 giants: 0
       tokens: -4028 ctokens: -4028



 4. flow table injection

    (1) check port number

      # ovs-ofctl show s1
      OFPT_FEATURES_REPLY (xid=0x2): dpid:0000000000aabbcc
      n_tables:254, n_buffers:256
      capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
      actions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst
       2(veth2): addr:fe:0b:d9:6c:31:c0
           config:     0
           state:      0
           current:    10GB-FD COPPER
           speed: 10000 Mbps now, 0 Mbps max
       3(veth4): addr:5a:70:9f:0a:6b:98
           config:     0
           state:      0
           current:    10GB-FD COPPER
           speed: 10000 Mbps now, 0 Mbps max
       4(veth0): addr:52:5e:be:01:2b:52
           config:     0
           state:      0
           current:    10GB-FD COPPER
           speed: 10000 Mbps now, 0 Mbps max
       LOCAL(s1): addr:00:00:00:aa:bb:cc
           config:     PORT_DOWN
           state:      LINK_DOWN
           speed: 0 Mbps now, 0 Mbps max
      OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0


    (2) generate flow table 
       # ovs-ofctl add-flow s1 ip,nw_src=10.1.1.5,actions=enqueue:4:1
               --> enqueue:4:1 means, 4(veth0)th port: 1st queue

         *** enqueue=<egress port>:<queue number>

        # ovs-ofctl dump-flows s1
          NXST_FLOW reply (xid=0x4):
           cookie=0x0, duration=27.596s, table=0, n_packets=0, n_bytes=0, idle_age=27, ip,nw_src=10.1.1.5 actions=enqueue:4:1
           cookie=0x0, duration=33987.016s, table=0, n_packets=5200328, n_bytes=110184305890, idle_age=46, priority=0 actions=NORMAL




 performance test
 ================

 1. use 'iperf' server-client measure
 2. capture results at each Host (H1, H2, H3)


        (1) no traffic shaping

          A. Server on H1 

            H1:~# iperf -s
            ------------------------------------------------------------
            Server listening on TCP port 5001
            TCP window size: 85.3 KByte (default)
            ------------------------------------------------------------
            [  4] local 10.1.1.1 port 5001 connected with 10.1.1.3 port 47806
            [ ID] Interval       Transfer     Bandwidth
            [  4]  0.0-10.0 sec  22.4 GBytes  19.2 Gbits/sec
            [  5] local 10.1.1.1 port 5001 connected with 10.1.1.5 port 37970
            [  5]  0.0-10.0 sec  25.0 GBytes  21.5 Gbits/sec


          B. from H3 (client)

            H3:~# iperf -c 10.1.1.1
            ------------------------------------------------------------
            Client connecting to 10.1.1.1, TCP port 5001
            TCP window size: 85.0 KByte (default)
            ------------------------------------------------------------
            [  3] local 10.1.1.5 port 37970 connected with 10.1.1.1 port 5001
            [ ID] Interval       Transfer     Bandwidth
            [  3]  0.0-10.0 sec  25.0 GBytes  21.5 Gbits/sec


          C. from H2 (client)
            H2:~# iperf -c 10.1.1.1
            ------------------------------------------------------------
            Client connecting to 10.1.1.1, TCP port 5001
            TCP window size: 85.0 KByte (default)
            ------------------------------------------------------------
            [  3] local 10.1.1.3 port 47806 connected with 10.1.1.1 port 5001
            [ ID] Interval       Transfer     Bandwidth
            [  3]  0.0-10.0 sec  22.4 GBytes  19.3 Gbits/sec



        (2) after setting qos and two queues and add flow into flow table on s1
                
        ** (note) q0 would be default queue

          A. Server on H1 

            H1:~# iperf -s
            ------------------------------------------------------------
            Server listening on TCP port 5001
            TCP window size: 85.3 KByte (default)
            ------------------------------------------------------------
            [  4] local 10.1.1.1 port 5001 connected with 10.1.1.5 port 37999
            [ ID] Interval       Transfer     Bandwidth
            [  4]  0.0-18.6 sec  4.25 MBytes  1.91 Mbits/sec
            [  5] local 10.1.1.1 port 5001 connected with 10.1.1.3 port 47840
            [  5]  0.0-10.0 sec  1.12 GBytes   956 Mbits/sec


          B. from H3 (client)

            H3:~# iperf -c 10.1.1.1
            ------------------------------------------------------------
            Client connecting to 10.1.1.1, TCP port 5001
            TCP window size: 85.0 KByte (default)
            ------------------------------------------------------------
            [  3] local 10.1.1.5 port 37999 connected with 10.1.1.1 port 5001
            [ ID] Interval       Transfer     Bandwidth
            [  3]  0.0-11.3 sec  4.25 MBytes  3.14 Mbits/sec



          C. from H2 (client)

            H2:~# iperf -c 10.1.1.1
            ------------------------------------------------------------
            Client connecting to 10.1.1.1, TCP port 5001
            TCP window size: 85.0 KByte (default)
            ------------------------------------------------------------
            [  3] local 10.1.1.3 port 47840 connected with 10.1.1.1 port 5001
            [ ID] Interval       Transfer     Bandwidth
            [  3]  0.0-10.0 sec  1.12 GBytes   958 Mbits/sec



        ** (Conclusion)
        the traffic from H3(10.1.1.5) uese lower speed queue- 2Mbps queue(q1) 
        whereas H2(10.1.1.3) uses 1000Mbps queue(q0)
        




 Useful Command for destroying queues and qos
 ============================================


 ** 1. clear qos and queues
        (1) first clear 
            ovs-vsctl clear qos veth0 queues

        (2) delete
            ovs-vsctl destroy qos <_uuid>
            ovs-vsctl --all destroy qos
            ovs-vsctl destroy queue <_uuid>
            ovs-vsctl --all destroy queue

        (3) if all methods failed to destroy, then delete port from ovs bridge
            ovs-vsctl del-port br0 eth1


 ** 2. create qos and queue

# ovs-vsctl set port veth0 qos=@newqos -- --id=@newqos create qos type=linux-htb queues=0=@q0,1=@q1 -- --id=@q0 create queue other-config:min-rate=1000000000 other-config:max-rate=1000000000 -- --id=@q1 create queue other-config:min-rate=2000000 other-config:max-rate=2000000




  
